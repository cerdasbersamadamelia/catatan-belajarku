# NLP-1 - Intuition

---

## ğŸ¯ Pengenalan NLP

### NLP sebagai "Magic Box" ğŸ©
- Model NLP (seperti ChatGPT) bisa dianggap sebagai **black box**
- Kita kasih pertanyaan â†’ model jawab â†’ seperti magic!
- Tapi ada proses di dalamnya yang perlu dipahami

### Fakta Penting: Komputer Cuma Ngerti Angka! ğŸ”¢
- Semua mesin (dari M3 Apple sampai Intel/AMD) **hanya mengerti angka**
- Komputer gak ngerti string, bahasa Inggris, atau bahasa Indonesia
- **Proses NLP:**
  1. Input (teks) â†’ Convert jadi angka
  2. Diproses sama model
  3. Output (angka) â†’ Convert balik jadi teks

---

## ğŸ“š Roadmap Pembelajaran NLP

Kita akan belajar NLP dari yang sederhana ke yang canggih:

1. **NLP tanpa Neural Network** (zaman dulu)
2. **Word Embedding** dan teman-temannya
3. **RNN & LSTM** (sebelum Transformer)
4. **Transformer & Attention Mechanism** (State of the art!)

### Materi Hari Ini:
1. âœ… Preprocessing
2. âœ… Tokenization
3. âœ… Word Embedding (dasar)

---

## ğŸ”„ Flow Kerja NLP Model

### Alur Umum ChatGPT:

```
User Input (String) 
    â†“
Convert ke Number
    â†“
ENCODER â†’ Pahami konteks input
    â†“
DECODER â†’ Generate jawaban
    â†“
Translate ke teks
    â†“
Jawaban (String)
```

**Encoder**: Tools untuk model mengerti input user
**Decoder**: Tools untuk generate output/jawaban

---

## ğŸ¨ Encoder & Embedding

### Tokenizer vs Embedding - Bedanya Apa?

#### Tokenizer
- **Fungsi**: Ubah teks jadi angka (ID token)
- **Output**: Angka ID sederhana
- **Contoh**: "Good morning" â†’ [839, 662]
- **Catatan**: Angka 0 dan 2 (awal & akhir) diabaikan dulu

#### Embedding 
- **Fungsi**: Representasi kata dalam bentuk **vektor angka**
- **Output**: Vektor panjang (misal 1024 dimensi)
- **Contoh**: "Good" â†’ [0.1757, -0.3421, 0.8901, ...]
- **Penting**: Ini yang dipahami model!

### Kenapa Perlu Embedding?

âŒ **Tokenizer aja gak cukup:**
- Gak bisa tahu kata "good" dan "better" itu mirip
- Gak bisa cari hubungan antar kata

âœ… **Embedding bisa:**
- Representasi kata dalam vektor
- Kata mirip â†’ vektor dekat
- Model bisa pahami konteks!

---

## ğŸ“Š Contoh Embedding dalam Action

### Percobaan dengan Model BART:

**Kata "Good" sendiri:**
```
good â†’ [0.1757, ...]
```

**"Good" dalam konteks berbeda:**
```
Good morning  â†’ [0.87, ...]
Good night    â†’ [0.8048, ...]
Very good     â†’ [0.1234, ...]
Good engineer â†’ [0.4348, ...]
```

**Kesimpulan**: Kata yang sama bisa punya vektor berbeda tergantung konteks!

---

## ğŸ§® Dimensi Embedding

### Apa itu Dimensi?
- **Dimensi** = panjang array/vektor
- Contoh: Shape `[1, 4, 1024]`
  - Batch: 1
  - Token: 4 (jumlah token)
  - Dimensi: 1024 (panjang vektor per token)

### Semakin Tinggi Dimensi:
âœ… **Pro**: Lebih akurat, representasi lebih kaya
âŒ **Cons**: Butuh komputasi lebih besar (CPU, RAM, GPU)

**Pilihan Dimensi:**
- GloVe: 50, 100, 200, 300
- BART: 1024
- BERT: 768

---

## ğŸŒ Word Embedding: GloVe

### Apa itu GloVe?
- **Global Vectors for Word Representation** (dari Stanford)
- Word embedding yang sudah jadi (pre-trained)
- Gak perlu training sendiri!

### Dataset GloVe:
- Wikipedia 2014 + Gigaword 5: **6 billion tokens**
- Common Crawl: **840 billion tokens**
- Sudah ditraining dengan algoritma khusus

### Cara Pakai GloVe:

```python
# Load GloVe
glove = load_glove('glove.6B.50d')

# Ambil embedding
embedding = glove['good']
# Output: [0.12099, -0.34521, ...]
```

### âš ï¸ Kekurangan GloVe:

1. **Angka Tetap (Statis)**
   - "Good morning" â†’ good = [0.12099, ...]
   - "Good night" â†’ good = [0.12099, ...] (sama!)
   - Gak peduli konteks

2. **Kalau Kata Gak Ada di Dataset â†’ Error!**
   - Kata bahasa daerah: âŒ
   - Typo: âŒ
   - Kata baru: âŒ

3. **Solusi**: Pakai Transformer (dinamis, pahami konteks!)

---

## ğŸ”¤ Tokenization

### Tokenization Dasar
- **Fungsi**: Pisah kalimat jadi token (kata/subkata)
- **Contoh sederhana**: Split by space
  ```
  "Good morning" â†’ ["Good", "morning"]
  ```

### BPE (Byte Pair Encoding) ğŸ”¥

**Keunggulan BPE:**
- Dipakai ChatGPT, BERT, dan model modern
- **Bisa handle kata yang gak ada di vocab!**

**Contoh Magic BPE:**
```
Input: "turutin ular"
Output: ["tur", "##ut", "##in", "ular"]
```

Tanda `##` = ada kata sebelumnya (lanjutan)

### ğŸ’¡ Tips Penting Tokenization:

#### 1. Pilih Model Sesuai Bahasa!

**Bahasa Indonesia dengan Model English:**
```
"Badan pemeriksaan keuangan"
â†’ 17 tokens ğŸ˜±
```

**Bahasa Indonesia dengan Model Indo:**
```
"Badan pemeriksaan keuangan"
â†’ 7 tokens âœ…
```

#### 2. Pakai Bahasa Inggris Lebih Hemat Token!

**Bahasa Indonesia:**
```
"Pemeriksa Keuangan" â†’ 7 tokens
```

**Bahasa Inggris:**
```
"Financial examiner" â†’ 2 tokens âœ¨
```

**Hemat token = Hemat biaya API!** ğŸ’°

---

## ğŸ§¹ Preprocessing Data Teks

### Kenapa Perlu Preprocessing?

Model lama (non-Transformer) butuh data **bersih** karena:
- Kalau kata gak ada di vocab â†’ gak jalan
- Noise (emoji, tagar, URL) ganggu akurasi

### Contoh Input Kotor:

```
"Super excited to share my latest article! ğŸ”¥ 
@OpenAI #AI #MachineLearning https://example.com"
```

### Langkah Cleaning:

#### 1. **Hapus Noise**
Hapus:
- URL (https://...)
- Mention (@username)
- Hashtag (#AI)
- Emoji (ğŸ”¥, ğŸ˜Š)
- Simbol khusus

**Hasil:**
```
"Super excited to share my latest article"
```

#### 2. **Hapus Tanda Baca**
```
"Super excited to share my latest article"
(tanda seru sudah hilang)
```

#### 3. **Hapus Double Space**
```
"Super excited to share my latest article"
(rapi!)
```

---

## âœ‚ï¸ Stemming vs Lemmatization

### Stemming

**Konsep**: Potong akhiran/awalan kata â†’ balik ke bentuk dasar

**Contoh:**
```
exciting  â†’ excite
happiness â†’ happi
worried   â†’ worri
```

**âœ… Kelebihan:**
- Cepat
- Bagus untuk regular verb (kata beraturan)

**âŒ Kekurangan:**
- **Gak bisa handle irregular verb!**
  ```
  went â†’ went (harusnya â†’ go) âŒ
  better â†’ better (harusnya â†’ good) âŒ
  best â†’ best (harusnya â†’ good) âŒ
  ```

---

### Lemmatization â­

**Konsep**: Ubah kata ke bentuk dasar yang **benar** secara linguistik

**Contoh:**
```
went â†’ go âœ…
better â†’ good âœ…
best â†’ good âœ…
hanging â†’ hang âœ…
```

**âœ… Kelebihan:**
- Lebih akurat
- Handle irregular verb
- Hasil lebih natural

**âŒ Kekurangan:**
- Butuh **Part of Speech (POS)** tagging
- Komputasi lebih berat

---

### Part of Speech (POS) Tagging

**Fungsi**: Tentukan jenis kata (kata benda, kerja, sifat, dll)

**Tag Bank:**
- **NN**: Noun (kata benda)
- **VB**: Verb (kata kerja)
- **JJ**: Adjective (kata sifat)
- **RB**: Adverb (kata keterangan)

**Contoh Proses:**

```python
Input: "The striped bats are hanging on their feet"

# Tokenize
tokens = ["The", "striped", "bats", "are", "hanging", ...]

# POS Tagging
pos = [("The", "DT"), ("striped", "JJ"), ("bats", "NNS"), ...]

# Lemmatization (pakai POS)
output = ["the", "striped", "bat", "be", "hang", ...]
```

---

## ğŸ¯ Cara Kerja Word Embedding (Intuisi)

### Visualisasi 2D:

Misalnya ada 2 dimensi: **Place** dan **Vehicle**

```
         Place  Vehicle
house     10      0      â†’ (10, 0)
car        0     10      â†’ (0, 10)
ice        0      0      â†’ (0, 0)
van        5     10      â†’ (5, 10) â† rumah + kendaraan!
train      5     10      â†’ (5, 10)
```

**Plot di grafik:**
```
Vehicle
  10 |    carâ—  vanâ—  trainâ—
     |
   5 |
     |
   0 |___iceâ—___________houseâ—
     0    5              10    Place
```

**Insight:**
- Kata dengan makna mirip â†’ posisi dekat
- Bisa dikelompokkan (clustering)
- Model bisa tahu hubungan kata!

---

### Embedding Dimension yang Tinggi

- Real embedding: **50-1024 dimensi** (gak bisa divisualisasi)
- Butuh **Aljabar Linear** untuk hitung jarak
- Metode: Cosine similarity, Euclidean distance, dll
- **Semakin dekat = semakin mirip makna**

---

## ğŸ¤– Text Classification (Sentimen Analysis)

### Konsep Dasar:

**Goal**: Klasifikasi teks â†’ Positif/Negatif/Netral

**Cara Kerja:**
1. Kata-kata di-embed jadi vektor
2. Hitung posisi vektor kata dalam "sentiment space"
3. Pakai **Naive Bayes** (statistical ML) untuk klasifikasi

**Contoh Sentiment Space:**

```
Positif (10, 0) â—
                 \
Netral (5, 5)     â— â† Kata "biasa"
                 /
Negatif (0, 10) â—
```

Kata "semangat" â†’ (9, 1) â†’ Dekat ke Positif! âœ…

---

## ğŸ“– Library NLP: NLTK

### Natural Language Toolkit

**Fungsi:**
- Preprocessing teks
- Stemming & Lemmatization
- POS Tagging
- Support **Bahasa Indonesia**! ğŸ‡®ğŸ‡©

**Alternatif untuk Bahasa Indonesia:**
- **Sastrawi** (lemmatization Indonesia, tapi development stopped)
- NLTK masih yang paling populer

---

## ğŸ“ Ringkasan & Key Takeaways

### NLP itu "Kompleks Kalkulator"
- Semua teks â†’ angka â†’ kalkulasi â†’ angka â†’ teks
- Embedding = representasi vektor dari kata
- Context matters! (Good morning â‰  Good night)

### Tokenization Penting!
- Pilih tokenizer sesuai bahasa dataset
- BPE bisa handle kata yang gak ada di vocab
- Pakai Bahasa Inggris untuk hemat token (API berbayar)

### Preprocessing Workflow:
1. **Cleaning**: Hapus noise (emoji, URL, hashtag)
2. **Tokenization**: Split jadi token
3. **Stemming/Lemmatization**: Balik ke kata dasar
4. **Embedding**: Convert ke vektor

### Model Selection:
- **Resource terbatas**: GloVe (50-300 dimensi)
- **Akurasi tinggi**: Transformer (768-1024 dimensi)
- **Bahasa Indonesia**: Cari model yang pre-trained di dataset Indonesia

---

## ğŸ” Cara Cek Dimensi Embedding Model

### Di Hugging Face:

1. Buka model page (misal: `facebook/bart-large`)
2. Klik **Files and versions**
3. Buka `config.json`
4. Cari:
   - `d_model` atau
   - `hidden_size`

**Contoh:**
```json
{
  "d_model": 1024,     // BART
  "hidden_size": 768   // BERT
}
```

---

## ğŸ’¡ Tips Pro

### 1. Pilih Model Bahasa Indonesia
**Cari di Hugging Face:**
- Filter: **Language â†’ Indonesian**
- Filter: **Task â†’ Text Classification / Feature Extraction**
- Lihat model card â†’ cek dataset training
- Contoh bagus: `IndoBERT`, `IndoLEM`, `IndoRoBERTa`

### 2. Cek Dataset Training
Model bagus biasanya dijelaskan:
- Dataset: Wikipedia Indonesia, Oscar, dll
- Jumlah data
- Bahasa yang dipakai

**Kalau gak dikasih tahu:**
- Baca paper-nya
- Atau langsung test dengan contoh teks!

### 3. Regex untuk Cleaning
Pakai regex untuk hapus noise:
```python
import re

# Hapus URL
text = re.sub(r'http\S+', '', text)

# Hapus mention
text = re.sub(r'@\w+', '', text)

# Hapus hashtag
text = re.sub(r'#\w+', '', text)

# Hapus emoji & simbol
text = re.sub(r'[^\w\s]', '', text)
```

---

## ğŸ“ Special Tokens

### Token Khusus di Model:

- **BOS** (Beginning of Sentence): Awal kalimat
- **EOS** (End of Sentence): Akhir kalimat
- **<s>**: Start (di beberapa model)
- **</s>**: Stop
- **[CLS]**: BERT start token
- **[SEP]**: BERT separator

**Catatan:**
- Otomatis ditambahkan tokenizer
- Gak perlu manual tulis
- Beda model beda special token (gak standar)

---

## ğŸš€ Pertemuan Selanjutnya

Materi yang akan dipelajari:

1. **Text Classification** (lanjutan)
   - Hitung manual pakai sheet
   - Implementasi Naive Bayes

2. **Word Embedding Methods:**
   - TF-IDF
   - Word2Vec
   - GloVe (detail)
   
3. **Praktik lebih dalam!**

---

## â“ FAQ & Troubleshooting

### Q: Tokenizer untuk LaTeX gimana?
**A**: 
- Model umum (ChatGPT) sudah support LaTeX
- Tapi gak optimal (banyak token)
- Kalau butuh optimal â†’ cari model khusus LaTeX
- Atau buat tokenizer sendiri

### Q: Kata gak ada di GloVe, gimana?
**A**: 
- GloVe = kamus, gak ada = error
- Solusi: Training ulang atau pakai Transformer

### Q: Stemming vs Lemmatization pilih mana?
**A**:
- **Stemming**: Kalau butuh cepat, resource terbatas
- **Lemmatization**: Kalau prioritas akurasi

### Q: Kenapa special token gak standar?
**A**: 
- Beda research lab, beda convention
- Gak ada standarisasi universal (yet)
- Always check `config.json`!

---

## ğŸ¯ Action Items

Setelah belajar materi ini, coba:

1. âœ… Download GloVe dan test dengan kata-kata sendiri
2. âœ… Bandingkan tokenizer BERT (English) vs IndoBERT
3. âœ… Praktik preprocessing dengan NLTK
4. âœ… Test stemming vs lemmatization
5. âœ… Eksplor Hugging Face untuk cari model Indonesia

---

**Catatan**: Materi ini fokus di **intuisi** NLP. Detail teknis (math, attention mechanism) akan dibahas di sesi selanjutnya!

ğŸ”¥ **Keep learning and happy coding!** ğŸš€
