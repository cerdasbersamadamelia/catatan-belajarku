# Deep Learning - Part 1

## ğŸ¯ Intro: Kenapa Deep Learning Seru?

Akhirnya masuk ke materi inti! Dari sini sampai akhir kita bakal belajar **Neural Network**. Bedanya sama traditional machine learning:
- **Machine Learning tradisional**: Banyak matematika rumit (kayak SVM, PCA)
- **Deep Learning**: Konsepnya lebih simpel - cuma **perkalian** sama **activation function**

### Real World Example: Subtitle Generator

Ada kasus nyata dari Ruang Guru - butuh bikin subtitle untuk video dalam waktu 1 minggu! Kalau manual butuh **40-50 hari** (6 jam/hari).

**Percobaan 1: Google Speech-to-Text (4-5 tahun lalu)**
- Hasilnya? KOCAK! ğŸ˜‚
- "tekuni" â†’ "teguni"
- "kepemimpinan" â†’ "kememinan"  
- "mindset" â†’ "perut" âŒ
- "bisnis" â†’ "bisa serius" âŒ

**Solusi: OpenAI Whisper**
- Hasilnya jauh lebih bagus!
- Cost: Cuma Rp 800 ribu untuk semua video
- Kesalahan cuma typo kecil (masih bisa ngeles!)
- Quality check bisa lebih cepat pakai ChatGPT

**Tools yang Bisa Dicoba:**
1. **Text-to-Speech & Speech-to-Text**: OpenAI API
2. **UI Version**: Google Cloud Console
3. **Tip QC Cepat**: Copy transcript â†’ Kasih ke LLM â†’ Minta identifikasi kesalahan

ğŸ’¡ **Motto kerja:** "Be lazy - kalau bisa cepat, kenapa harus kerja keras?"

---

## ğŸ“š Recap Machine Learning Sebelumnya

Semua model ML punya pola yang sama:
- Ada **Loss Function** (yang mau di-minimize atau maximize)
- Ada **Algorithm** untuk mencapai nilai optimal

**Contoh:**
- **Decision Tree**: Loss function = Gini Entropy
- **SVM**: Maximize margin (jarak garis ke titik-titik)

---

## ğŸ§  Konsep Dasar Neural Network

### Persamaan Linear Sederhana

```
y = mx + b
```

Di Machine Learning:
- **x** = input (contoh: tinggi badan)
- **y** = output (contoh: berat badan)
- **m** = slope/kemiringan
- **b** = bias/intercept

**Multiple Input:**
```
y = axâ‚ + bxâ‚‚ + c
```
- xâ‚ = tinggi badan
- xâ‚‚ = usia
- y = berat badan

Dalam Neural Network:
- **a, b** = **weights** (bobot)
- **c** = **bias**

### Linear Regression: Cara Kerja

1. Punya data points random
2. Bikin garis lurus yang paling pas
3. Hitung **loss** = jarak antara garis prediksi vs titik sebenarnya
4. Gunakan **MSE** (Mean Squared Error): rata-rata jarak dikuadratkan
5. Loop terus sampai dapat garis terbaik!

**Proses:**
- Model nyoba-nyoba nilai **weight (w)** dan **bias (b)**
- Di setiap iterasi (epoch), loss makin turun
- Contoh: Epoch 100 â†’ loss 0.7, Epoch 200 â†’ loss 0.4 (makin bagus!)

### Epoch: Berapa Kali Training?

- **Epoch 10**: Garis masih jelek
- **Epoch 50**: Mulai bagus
- **Epoch 100**: Lebih bagus lagi
- **Epoch 200**: Udah optimal

âš ï¸ **Tapi hati-hati:** Epoch terlalu tinggi â†’ **overfitting**

---

## ğŸ¨ Visualisasi Neural Network

### Arsitektur Sederhana

```
Input â†’ [Weight] â†’ (+Bias) â†’ Output
  x    Ã—   w          + b    â†’   y
```

**Contoh:**
- Input: 2
- Weight: 3
- Bias: 8
- Output: (2 Ã— 3) + 8 = **14**

### Network dengan Multiple Layers

```
Input Layer â†’ Hidden Layer â†’ Output Layer
   xâ‚€            zâ‚€             y
   xâ‚            zâ‚
```

**Fully Connected:** Setiap input nyambung ke semua neuron di layer berikutnya!

---

## ğŸ”¢ Matematika Neural Network

### Input & Output

**1 Input, 1 Neuron:**
- Input: xâ‚€, xâ‚
- Weights: wâ‚€, wâ‚
- Bias: b
- Output: z = (wâ‚€ Ã— xâ‚€) + (wâ‚ Ã— xâ‚) + b

### Hitung Jumlah Weights

**Quiz:**
- Input: 2, Neurons: 5 â†’ Weights = **2 Ã— 5 = 10**
- Bias = **5** (satu per neuron)
- Tambah layer kedua (3 neurons) â†’ Total weights = **10 + 15 = 25**, Total bias = **8**

### Representasi Vector

Daripada nulis panjang, pakai **vector**!

```
z = WÂ·X + B
```

Di mana:
- **X** = vector input [xâ‚€, xâ‚]
- **W** = vector weights [wâ‚€, wâ‚]
- **B** = vector bias
- **z** = output

**Kenapa pakai vector?**
- Lebih simpel
- Performance lebih bagus
- Scalable (gampang ditambah layer/neuron)

---

## ğŸš€ Multiple Layers & Matrix

### 2 Input, 2 Neurons

```
Input [xâ‚€, xâ‚] â†’ [zâ‚€, zâ‚]
```

**Weights jadi Matrix 2Ã—2:**
```
W = [wâ‚€â‚€  wâ‚€â‚]
    [wâ‚â‚€  wâ‚â‚]
```

- Row pertama = weights masuk ke zâ‚€
- Row kedua = weights masuk ke zâ‚

### Deep Neural Network (2 Layers)

**Layer 1:**
```
zâ½â°â¾ = Wâ½â°â¾Â·X + Bâ½â°â¾
```

**Layer 2:**
```
zâ½Â¹â¾ = Wâ½Â¹â¾Â·zâ½â°â¾ + Bâ½Â¹â¾
```

**Gabungan:**
```
zâ½Â¹â¾ = Wâ½Â¹â¾Â·(Wâ½â°â¾Â·X + Bâ½â°â¾) + Bâ½Â¹â¾
```

ğŸ’¡ Tapi kita gak perlu nulis panjang - cukup panggil fungsi matrix multiplication!

---

## âš¡ Activation Function - Kunci Nonlinearity!

### Kenapa Butuh Activation Function?

**Masalah:** Data real gak selalu linear (garis lurus)!

Contoh data yang **GAK BISA** dipisahkan garis lurus:
- Data spiral ğŸŒ€
- Data melingkar â­•
- Data bentuk XOR

**Solusi:** Tambah **Activation Function** setelah setiap layer!

```
Input â†’ [Weight Ã— Input + Bias] â†’ Activation Function â†’ Output
```

### Jenis-Jenis Activation Function

#### 1. **Linear** (Default - Gak Berguna!)
- Output = Input
- Gak bisa handle data nonlinear
- âš ï¸ Kalau semua layer pakai linear = sama aja cuma 1 neuron!

#### 2. **ReLU** (Rectified Linear Unit) â­ POPULER
```
ReLU(z) = max(0, z)
```
- Input < 0 â†’ Output = 0
- Input â‰¥ 0 â†’ Output = Input
- **Paling sering dipakai!**

**Contoh:**
- ReLU(5) = 5
- ReLU(-1) = 0
- ReLU(10) = 10
- ReLU(-10) = 0

#### 3. **Leaky ReLU**
```
LeakyReLU(z) = max(0.01z, z)
```
- Mirip ReLU, tapi ada "bocor" dikit di bagian negatif
- Slope negatif = 0.01 (gak benar-benar 0)

**Contoh:**
- Input = -2 â†’ Output = -0.02

#### 4. **Sigmoid**
```
Ïƒ(z) = 1 / (1 + e^(-z))
```
- Output range: **0 sampai 1**
- Bentuk huruf S
- Bagus untuk **probabilitas**

#### 5. **Tanh** (Hyperbolic Tangent)
- Output range: **-1 sampai 1**
- Mirip sigmoid tapi centered di 0
- Lebih bagus dari sigmoid di banyak kasus

---

## ğŸ® Eksperimen di Playground

**Website:** playground.tensorflow.org

### Challenge 1: Data Melingkar
**Solusi:**
- 2 hidden layers
- Activation: **Tanh** atau **ReLU**
- Neurons: 3-4 per layer

### Challenge 2: Data XOR
**Solusi:**
- 1-2 hidden layers
- 4-8 neurons
- Activation: **ReLU**
- Bisa juga pakai **feature engineering** (xÂ², yÂ²)

### Challenge 3: Data Spiral ğŸŒ€ (SUSAH!)
**Solusi:**
- 8+ neurons per layer
- 3+ hidden layers
- Activation: **ReLU**
- Test loss bisa 0.002!

ğŸ’¡ **Pro Tips:**
- Kalau simple network bisa solve â†’ pakai yang simple!
- Feature engineering bisa bantu simplify network
- Lebih banyak neurons/layers â‰  selalu lebih bagus

---

## ğŸ”¥ Kenapa Activation Function Penting?

### Tanpa Activation Function

```
Multiple layers tanpa activation = Cuma 1 layer doang!
```

**Bukti:**
```
Layer 1: zâ‚ = Wâ‚X + Bâ‚
Layer 2: zâ‚‚ = Wâ‚‚zâ‚ + Bâ‚‚
       = Wâ‚‚(Wâ‚X + Bâ‚) + Bâ‚‚
       = (Wâ‚‚Wâ‚)X + (Wâ‚‚Bâ‚ + Bâ‚‚)
       = W'X + B'  â† Cuma linear biasa!
```

**Dengan Activation Function:**
```
zâ‚ = ReLU(Wâ‚X + Bâ‚)  â† Nonlinear!
zâ‚‚ = ReLU(Wâ‚‚zâ‚ + Bâ‚‚) â† Nonlinear lagi!
```

Sekarang bisa bikin garis melengkung! ğŸ‰

---

## ğŸ–¼ï¸ Aplikasi Real: Image Classification

Contoh: Deteksi gambar (Coffee, Red Panda, Orange)

**Arsitektur:**
```
Input Image â†’ Layer 1 â†’ Layer 2 â†’ ... â†’ Layer N â†’ Output
  (RGB)         Hidden    Hidden         Hidden    (Classes)
```

**Output Layer:**
- Setiap class punya nilai
- Class dengan nilai **tertinggi** = prediksi!

**RGB vs Grayscale:**
- **RGB**: 3 channel (Red, Green, Blue) - lebih informatif
- **Grayscale**: 1 channel (hitam-putih) - susah bedain warna

---

## ğŸ“ˆ Loss Function

**MSE** (Mean Squared Error) untuk regression:
```
Loss = 1/n Ã— Î£(y_pred - y_actual)Â²
```

Tujuan training: **Minimize loss!**

---

## ğŸ’¡ Fun Facts & Tips

### Sejarah Neural Network
- Neural network **sempat hampir mati** ğŸ’€
- Hidup lagi karena:
  1. **Big Data** (internet = unlimited data)
  2. **Cloud Computing** (komputasi murah)

### Secret of Deep Learning Success
**Ternyata simple:** Tambahin data = hasil makin bagus! ğŸ˜…

Contoh: GPT
- GPT-1: Data dikit
- GPT-2: Data lebih banyak
- GPT-3: **175 billion parameters** + data super banyak!
- GPT-4: ...you know the drill

### GPT-3 Specs
- **96 layers** (dalem banget!)
- **175 billion parameters**
- Konsepnya sama persis: Input â†’ Weights â†’ Activation â†’ Output

---

## ğŸ¯ Challenge untuk Kamu!

**Pertanyaan Menarik:** Kenapa pakai **Sigmoid gak bisa** solve data spiral, tapi **ReLU bisa**?

ğŸ’­ **Hint:** Bukan karena range output (0-1 vs 0-âˆ)
ğŸ” **Cari tahu kenapa!** Akan dibahas sesi depan!

---

## ğŸ“ Summary

âœ… **Neural Network = Input â†’ Weight Ã— Input + Bias â†’ Activation â†’ Output**
âœ… **Linear aja gak cukup - butuh activation function!**
âœ… **ReLU = activation function favorit** (simple tapi powerful)
âœ… **Multiple layers + activation = bisa solve masalah kompleks**
âœ… **Weights & bias = yang di-training** (berubah tiap epoch)
âœ… **Vector/Matrix = cara simpel representasi perhitungan**
âœ… **Lebih banyak data + compute power = hasil lebih bagus**

---

## ğŸ”œ Next Session

Kita akan bahas:
- **Backpropagation** (cara neural network belajar)
- **Detail activation function** (kenapa sigmoid gak work?)
- **Overfitting & Regularization**
- **Convolutional Neural Network (CNN)**

Stay tuned! ğŸš€
