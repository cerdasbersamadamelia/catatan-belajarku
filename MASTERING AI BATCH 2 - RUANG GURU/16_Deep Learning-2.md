# Deep Learning 2 - Matematika di Balik Neural Network

---

## ğŸ¯ Overview

Sesi ini fokus pada **matematika di balik Deep Learning**, khususnya:
- Kalkulus (Turunan/Derivative)
- Gradient Descent
- Backpropagation
- Loss Function Optimization

---

## ğŸ“Œ Recap: Neural Network Basics

### Single Neuron - Single Input
```
Input (x=2) â†’ [Neuron (w=3, b=1)] â†’ Output
```

**Perhitungan:**
- 2 Ã— 3 = 6
- 6 + 1 = 7... tunggu, ada bias yang dikurangi!
- 2 Ã— 3 = 6, lalu 6 - 1 = **5**

**Output: 5**

---

### Multiple Input - Single Neuron
```
Input 1 (xâ‚=1) â†’ wâ‚=-2 â”
                       â”œâ†’ [Neuron (b=3)] â†’ Output
Input 2 (xâ‚‚=2) â†’ wâ‚‚=3  â”˜
```

**Perhitungan:**
- (1 Ã— -2) + (2 Ã— 3) + 3
- -2 + 6 + 3 = **7**

**Output: 7**

---

### Multiple Input - Multiple Neurons
```
Input 1 (xâ‚=1) â†’ wâ‚=2, wâ‚ƒ=3    â”
                                â”œâ†’ Neuron 1 (bâ‚=2) â†’ Output 1
Input 2 (xâ‚‚=2) â†’ wâ‚‚=5, wâ‚„=-2   â”œâ†’ Neuron 2 (bâ‚‚=2) â†’ Output 2
                                â”˜
```

**Neuron 1:**
- (1 Ã— 2) + (2 Ã— 5) + 2
- 2 + 10 + 2 = **14**

**Neuron 2:**
- (1 Ã— 3) + (2 Ã— -2) + 2
- 3 - 4 + 2 = **1**

**Output: (14, 1)**

---

## ğŸ¬ Activation Function

### ReLU (Rectified Linear Unit)
```
f(x) = max(0, x)
```

**Karakteristik:**
- Jika x > 0 â†’ output = x
- Jika x â‰¤ 0 â†’ output = 0

**Contoh 1:**
```
Input=2, w=3, b=-1 â†’ 2Ã—3-1 = 5 â†’ ReLU(5) = 5
```

**Contoh 2:**
```
Input=2, w=-3, b=-1 â†’ 2Ã—(-3)-1 = -7 â†’ ReLU(-7) = 0
```

---

## ğŸ¤– Fun Fact: LLaMA (Meta)

### Kenapa LLaMA Menarik?
- **Open Source** dari Meta (Facebook/Instagram/WhatsApp)
- Bisa dijalankan di komputer lokal!
- Code cuma **500 baris** (C/C++)
- Yang besar itu **parameter**-nya (weights & biases): **140 GB!**

### Magic-nya di Mana?
```
Code: 500 lines â† Kecil!
Parameters: 140 GB â† BESAR! (ini isinya weights & biases)
```

**ChatGPT kerja gimana?**  
- Predict kata **per kata** (token per token)
- Dibikin seolah ngetik pelan biar keliatan keren
- Tapi sebenarnya emang prediksi satu-satu

---

## ğŸ† Chatbot Arena Leaderboard

**Top Models:**
1. GPT-4 (Closed Source)
2. Claude (Closed Source)
3. **LLaMA** (Open Source) â† Naik terus!

**Kenapa Model Besar Lebih Bagus?**
- Makin banyak neurons â†’ Loss makin kecil
- Makin banyak data training â†’ Makin akurat
- Tapi... butuh GPU mahal!

**Kenapa Ada Model Kecil?**
- Bisa jalan di HP! (contoh: Gemini Nano)
- Lebih murah
- Tidak perlu internet
- Cocok untuk task sederhana

---

## ğŸ“ Kalkulus: Derivative (Turunan)

### Aturan Dasar
```
f(x) = 2xÂ³ + 3xÂ² + 2x + 1
```

**Cara Menurunkan:**
- Pangkat Ã— koefisien
- Pangkat turun 1

```
f'(x) = 3Ã—2xÂ² + 2Ã—3xÂ¹ + 1Ã—2xâ° + 0
f'(x) = 6xÂ² + 6x + 2
```

---

### Tapi... Kenapa Begitu?

**Definisi Turunan:**
> Turunan = **Kemiringan garis singgung** di suatu titik

---

## ğŸ“Š Visualisasi: Garis Singgung

```
      â•±
    â•±
  â•±  â† Garis singgung (merah)
 â€¢
â•± â•²
   â•² â† Kurva asli (biru)
```

**Garis Singgung:**
- Bentuknya: `y = mx + c`
- **m** = kemiringan (slope)
- **c** = konstanta

**Cara Cari Kemiringan:**
1. Ambil 2 titik yang dekat
2. Hitung: `m = Î”y / Î”x`
3. Makin dekat titiknya, makin akurat!

---

### Contoh Perhitungan Slope

**Titik 1:** (2, 13)  
**Titik 2:** (3, 38)

```
m = (38 - 13) / (3 - 2)
m = 25 / 1
m = 25
```

**Makin dekat, makin akurat:**
- Jarak 1.0 â†’ kurang akurat
- Jarak 0.5 â†’ lebih akurat
- Jarak 0.1 â†’ sangat akurat
- Jarak â†’ 0 â†’ **PERFECT!** â† Ini definisi limit!

---

## ğŸ”¢ Limit & Derivative

### Definisi Formal
```
dy/dx = lim(Î”xâ†’0) [f(x + Î”x) - f(x)] / Î”x
```

**Artinya:**
- Î”x makin kecil mendekati 0
- Slope makin mendekati nilai sebenarnya

---

### Contoh: Turunan xÂ²

**Given:** `f(x) = xÂ²`

**Langkah 1:** Masukkan ke rumus limit
```
dy/dx = lim(Î”xâ†’0) [(x + Î”x)Â² - xÂ²] / Î”x
```

**Langkah 2:** Jabarkan
```
= lim(Î”xâ†’0) [xÂ² + 2xÂ·Î”x + Î”xÂ² - xÂ²] / Î”x
```

**Langkah 3:** Coret yang sama
```
= lim(Î”xâ†’0) [2xÂ·Î”x + Î”xÂ²] / Î”x
```

**Langkah 4:** Bagi semua dengan Î”x
```
= lim(Î”xâ†’0) [2x + Î”x]
```

**Langkah 5:** Î”x â†’ 0
```
= 2x
```

**Terbukti!** Turunan xÂ² adalah **2x** âœ…

---

## ğŸ§® Aturan Turunan

### 1. Penjumlahan
```
d/dx[f(x) + g(x)] = f'(x) + g'(x)
```

**Contoh:**
```
f(x) = xÂ² + 2x
f'(x) = 2x + 2
```

---

### 2. Perkalian (Product Rule)
```
d/dx[f(x) Â· g(x)] = f'(x)Â·g(x) + f(x)Â·g'(x)
```

**Contoh:**
```
f(x) = xÂ² Â· 2x = 2xÂ³
f'(x) = (2x)Â·(2x) + (xÂ²)Â·(2) = 4xÂ² + 2xÂ² = 6xÂ²
```

---

### 3. Pangkat
```
d/dx[xâ¿] = nÂ·xâ¿â»Â¹
```

**Contoh:**
```
f(x) = xÂ³ â†’ f'(x) = 3xÂ²
f(x) = xâµ â†’ f'(x) = 5xâ´
```

---

### 4. Chain Rule (Aturan Rantai)
```
dy/dx = (dy/du) Â· (du/dx)
```

**Penting banget untuk Backpropagation!**

---

## ğŸ¯ Gradient Descent

### Konsep: Turun Gunung

Bayangkan kamu di puncak gunung, mau turun ke lembah (titik minimum):

```
      ğŸ”ï¸
     â•±  â•²
    â•±    â•²
   â•±      â•²
  â•±   ğŸ¯   â•²
 â•±          â•²
```

**Pertanyaan:** Gimana caranya turun?
1. Cek kemiringan (slope)
2. Jalan ke arah yang turun
3. Ulangi sampai sampai dasar

---

### Formula Gradient Descent

```
x_new = x_old - Î± Â· (dy/dx)
```

**Di mana:**
- **x_old** = posisi sekarang
- **Î±** (alpha) = learning rate (ukuran langkah)
- **dy/dx** = slope (kemiringan)
- **x_new** = posisi baru

---

### Contoh: Cari Minimum f(x) = xÂ² - 6x + 14

**Langkah 1:** Turunan
```
f'(x) = 2x - 6
```

**Langkah 2:** Mulai dari x = 10, Î± = 0.1
```
Iterasi 1:
slope = 2(10) - 6 = 14
x_new = 10 - 0.1Ã—14 = 8.6

Iterasi 2:
slope = 2(8.6) - 6 = 11.2
x_new = 8.6 - 0.1Ã—11.2 = 7.48

... (lanjut terus)

Iterasi 20:
x â‰ˆ 3.0 â† MINIMUM! ğŸ¯
```

---

## ğŸ“‰ Learning Rate (Î±)

### Learning Rate Terlalu Kecil (Î± = 0.01)
```
âŒ Terlalu lambat!
   Butuh 200 iterasi
   Kayak semut turun gunung
```

### Learning Rate Pas (Î± = 0.1)
```
âœ… Optimal!
   Cukup 20 iterasi
   Cepat & akurat
```

### Learning Rate Terlalu Besar (Î± = 0.8)
```
âš ï¸ Masih oke...
   Lebih cepat dari Î±=0.1
```

### Learning Rate Kebesaran (Î± = 1.5)
```
âŒ BAHAYA!
   Malah bounce sana-sini
   Tidak konvergen
   Bisa diverge (makin jauh dari minimum)
```

---

## ğŸ¨ Visualisasi Learning Rate

```
Î± terlalu kecil:
ğŸŒ â†’ â†’ â†’ â†’ â†’ â†’ â†’ â†’ â†’ ğŸ¯ (lambat banget)

Î± optimal:
ğŸƒ â†’ â†’ â†’ ğŸ¯ (pas!)

Î± terlalu besar:
ğŸš€ â†—ï¸ â†˜ï¸ â†—ï¸ â†˜ï¸ â†—ï¸ â†˜ï¸ (bounce terus!)
```

---

## ğŸ§ª Aplikasi ke Neural Network

### Problem: NOT Gate

**Training Data:**
```
Input | Expected Output
------|----------------
  0   |       1
  1   |       0
```

**Neural Network:**
```
x â†’ [w, b] â†’ z â†’ output
```

**Goal:** Cari w dan b yang tepat!

---

### Manual (Trial & Error)

```
Coba w=2, b=3:
- Input 0: 0Ã—2+3 = 3 â†’ sigmoid â†’ 0.95 âŒ (harusnya 1)
- Input 1: 1Ã—2+3 = 5 â†’ sigmoid â†’ 0.99 âŒ (harusnya 0)
Cost = 0.99 (jelek!)

Coba w=-1, b=1:
- Input 0: 0Ã—(-1)+1 = 1 âœ…
- Input 1: 1Ã—(-1)+1 = 0 âœ…
Cost = 0.00 (PERFECT!)
```

**Tapi manual itu capek!** ğŸ˜«

---

### Otomatis: Gradient Descent!

**Cost Function:**
```
C = (y_expected - y_predicted)Â²
```

**Yang Perlu Dicari:**
1. âˆ‚C/âˆ‚w (slope terhadap weight)
2. âˆ‚C/âˆ‚b (slope terhadap bias)

---

## ğŸ”— Backpropagation

### Arsitektur
```
x â†’ [w] â†’ z â†’ a â†’ C
     â†‘    â†‘   â†‘   â†‘
     |    |   |   |
    weight z  activation  cost
```

**Problem:** Gimana cari âˆ‚C/âˆ‚w kalau jaraknya jauh?

**Solusi:** **Chain Rule!**

---

### Chain Rule untuk Backprop

**Mau cari:** âˆ‚C/âˆ‚w

**Tapi C jauh dari w, jadi:**
```
âˆ‚C/âˆ‚w = (âˆ‚C/âˆ‚z) Â· (âˆ‚z/âˆ‚w)
```

**Atau kalau ada activation:**
```
âˆ‚C/âˆ‚w = (âˆ‚C/âˆ‚a) Â· (âˆ‚a/âˆ‚z) Â· (âˆ‚z/âˆ‚w)
```

**Inilah "chaining" backward dari output ke input!**

---

### Detail Perhitungan

**Given:**
- z = wx + b
- a = sigmoid(z)
- C = (y - a)Â²

**Step 1: âˆ‚C/âˆ‚a**
```
C = (y - a)Â²
âˆ‚C/âˆ‚a = -2(y - a) = 2(a - y)
```

**Step 2: âˆ‚a/âˆ‚z**
```
a = sigmoid(z)
âˆ‚a/âˆ‚z = sigmoid'(z) = a(1-a)
```

**Step 3: âˆ‚z/âˆ‚w**
```
z = wx + b
âˆ‚z/âˆ‚w = x
```

**Gabungkan (Chain Rule):**
```
âˆ‚C/âˆ‚w = âˆ‚C/âˆ‚a Â· âˆ‚a/âˆ‚z Â· âˆ‚z/âˆ‚w
âˆ‚C/âˆ‚w = 2(a-y) Â· a(1-a) Â· x
```

**Sama untuk bias:**
```
âˆ‚C/âˆ‚b = 2(a-y) Â· a(1-a) Â· 1
```

---

### Update Weight & Bias

```
w_new = w_old - Î± Â· (âˆ‚C/âˆ‚w)
b_new = b_old - Î± Â· (âˆ‚C/âˆ‚b)
```

**Ulangi sampai Cost â†’ 0!**

---

## ğŸ“Š Simulasi Training

### Epoch 1
```
Input: x=0
w=2, b=3
z = 0Ã—2+3 = 3
a = sigmoid(3) = 0.95
Expected: 1
Cost: (1-0.95)Â² = 0.0025

âˆ‚C/âˆ‚w = ...hitung... = -0.13
âˆ‚C/âˆ‚b = ...hitung... = -0.07

Update:
w = 2 - 0.1Ã—(-0.13) = 2.013
b = 3 - 0.1Ã—(-0.07) = 3.007
```

### Epoch 2
```
w=2.013, b=3.007
Cost turun â†’ 0.97
Masih belum bagus, lanjut!
```

### Epoch 20
```
w â‰ˆ -1.0
b â‰ˆ 1.0
Cost â‰ˆ 0.04
Sudah lumayan!
```

### Epoch 100
```
w = -1.0
b = 1.0
Cost â‰ˆ 0.00
PERFECT! ğŸ‰
```

---

## ğŸŒ„ Cost Function Landscape

### 2D (1 Parameter)
```
  Cost
    |     â•±â•²
    |    â•±  â•²
    |   â•±    â•²
    |  â•±  ğŸ¯  â•²
    |_â•±________â•²___
           w
```

### 3D (2 Parameters: w & b)
```
       Cost
        â†‘
        |     â•±â•²
        |    â•±  â•²
        |   â•± ğŸ¯ â•²
        |__â•±______â•²___â†’ w
        â•±
       b
```

**Kita cari titik paling bawah (ğŸ¯)!**

---

## âš ï¸ Local Minima Problem

```
       â•±â•²         â•±â•²
      â•±  â•²       â•±  â•²
     â•± ğŸ”´ â•²     â•± ğŸ¯ â•²
    â•±______â•²___â•±______â•²
   Local Min   Global Min
```

**Problem:** Bisa Ğ·Ğ°ÑÑ‚Ñ€ÑÑ‚ÑŒ di local minimum!

**Solusi:**
1. Coba learning rate berbeda
2. Multiple random start
3. Adaptive learning rate
4. Advanced optimizer (Adam, RMSprop)

**Fun Fact:** Di high-dimension, local minima jarang terjadi!

---

## ğŸ“ Key Takeaways

### 1. Turunan = Kemiringan
- Dipakai untuk tahu arah mana yang turun
- Makin curam, makin cepat turun

### 2. Gradient Descent = Turun Gunung
- Mulai dari titik random
- Cek kemiringan
- Jalan sedikit ke arah turun
- Repeat!

### 3. Learning Rate = Ukuran Langkah
- Terlalu kecil â†’ lambat
- Terlalu besar â†’ bounce
- Harus pas!

### 4. Backpropagation = Chain Rule
- Turunan dari output ke input
- Backward pass untuk update weight
- Ini yang bikin neural network bisa "belajar"

### 5. Cost Function = Target Optimasi
- Ukuran seberapa salah prediksi
- Goal: bikin cost sekecil mungkin
- Pakai gradient descent untuk minimize

---

## ğŸ’¡ Tips Praktis

### âœ… DO:
- Pahami konsep, bukan cuma rumus
- Visualisasikan (gambar kurva, gunung, dll)
- Eksperimen dengan learning rate
- Mulai dari case sederhana (NOT gate)

### âŒ DON'T:
- Jangan hafalkan rumus tanpa paham
- Jangan skip matematika (penting!)
- Jangan pakai learning rate sembarangan
- Jangan takut mencoba!

---

## ğŸ”§ Praktik: Google Sheets

**Link ada di materi!**

Fitur:
- Simulasi gradient descent
- Manual tuning w & b
- Visualisasi cost function
- Lihat langkah per langkah

**Coba sendiri:**
1. Ubah learning rate
2. Ubah starting point
3. Lihat berapa iterasi sampai konvergen
4. Eksperimen dengan activation function

---

## ğŸ“š Resources Tambahan

### Video Recommended:
- **3Blue1Brown:** Neural Network series (visualisasi keren!)
- Chain rule & backpropagation explained
- Gradient descent visualization

### Tools:
- Google Sheets (ada di materi)
- TensorFlow Playground
- Desmos (untuk plot fungsi)

---

## ğŸ¯ Project Mendatang

### Project 2: Machine Learning
- **Deadline:** Jumat, 26 Januari (tengah malam)
- Sudah bisa diakses di LMS

### Project 3: Deep Learning
- **Deadline:** Minggu depan (tengah malam)
- Akan dibahas di pertemuan berikutnya

**Catatan:** Project akan menggunakan library (TensorFlow/PyTorch), jadi tidak perlu implement gradient descent manual. Tapi **PENTING** untuk paham konsepnya!

---

## ğŸ¤” FAQ

**Q: Apakah harus hapal semua rumus?**  
A: Tidak! Yang penting paham konsep. Library sudah handle semua perhitungan.

**Q: Kenapa harus belajar matematika kalau sudah ada library?**  
A: Supaya tahu "magic" di baliknya! Biar bisa debug, optimize, dan paham kenapa model tidak bagus.

**Q: Apakah selalu bentuk mangkok cost function?**  
A: Tidak selalu. Bisa ada local minima. Tapi di high-dimension, jarang stuck.

**Q: Learning rate optimal berapa?**  
A: Tergantung problem! Biasanya 0.001 - 0.1. Harus trial & error atau pakai adaptive optimizer.

---

## ğŸš€ Next Steps

1. âœ… Review materi ini
2. âœ… Main-main di Google Sheets
3. âœ… Tonton video 3Blue1Brown
4. âœ… Kerjakan Project 2
5. âœ… Siap untuk pertemuan berikutnya!

---

## ğŸ“ Catatan Penting

> "Matematika di deep learning itu seperti mesin di mobil. Kamu tidak harus jadi mekanik untuk bisa nyetir, tapi kalau paham cara kerja mesin, kamu bisa jadi driver yang lebih baik!"

**Intinya:**
- Library sudah handle semua
- Tapi paham konsep = POWER! ğŸ’ª
- Gradient Descent = Inti dari learning
- Backpropagation = Chain rule applied
- Turunan = Kemiringan = Arah turun

---

**Happy Learning! ğŸ‰**
