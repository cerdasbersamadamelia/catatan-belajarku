# CNN - Part 1 (Convolutional Neural Networks)

## Apa itu CNN?

CNN (Convolutional Neural Networks) adalah jenis deep learning yang dirancang khusus untuk memproses data yang berbentuk grid, terutama gambar.

**Kenapa CNN untuk gambar?**
- Mampu menangkap spatial information (posisi pixel)
- Lebih efisien daripada Fully Connected Network
- Weight sharing → parameter lebih sedikit
- Translation invariant (deteksi objek di posisi mana pun)

## Perbandingan dengan Neural Network Biasa

### Fully Connected Network
**Masalah:**
- Gambar 224x224x3 = 150,528 input
- Hidden layer 1000 neurons = 150 juta parameter!
- Overfitting dan training sangat lambat
- Tidak capture spatial information

### CNN
**Keuntungan:**
- Parameter lebih sedikit (weight sharing)
- Capture local patterns
- Hierarchical feature learning
- Translation equivariant

## Arsitektur CNN

### 1. Convolutional Layer

**Konsep:**
- Menggunakan filter/kernel untuk extract features
- Filter slide (convolve) across image
- Menghasilkan feature map

**Filter/Kernel:**
- Matrix kecil (3x3, 5x5, 7x7)
- Learned parameters (weights)
- Detect specific patterns (edges, textures, shapes)

**Contoh Filter 3x3:**
```
[1  0 -1]
[1  0 -1]
[1  0 -1]
```
Filter ini detect vertical edges.

**Convolution Operation:**
1. Overlay filter pada bagian image
2. Element-wise multiplication
3. Sum semua hasil
4. Geser filter (stride)
5. Repeat

**Parameter:**
- **Kernel size:** ukuran filter (3x3, 5x5)
- **Stride:** langkah geser filter (biasanya 1)
- **Padding:** tambahan border (0 atau 'same')
- **Number of filters:** berapa banyak feature maps

**Output size:**
```
Output = (Input - Kernel + 2*Padding) / Stride + 1
```

Contoh:
- Input: 32x32
- Kernel: 3x3
- Stride: 1
- Padding: 0
- Output: (32 - 3) / 1 + 1 = 30x30

### 2. Activation Function

**ReLU (Rectified Linear Unit):**
```python
f(x) = max(0, x)
```
- Simple dan cepat
- Mengatasi vanishing gradient
- Paling populer di CNN

**Kenapa perlu activation?**
- Nonlinearity
- Tanpa activation, CNN cuma linear transformation
- ReLU membuat network bisa belajar complex patterns

### 3. Pooling Layer

**Tujuan:**
- Reduce spatial dimensions (downsampling)
- Reduce parameters dan computation
- Make representation more robust

**Max Pooling:**
- Ambil nilai maximum dari region
- Contoh: 2x2 max pooling dengan stride 2
```
Input 4x4:
[1  3  2  4]
[5  6  7  8]
[3  2  1  0]
[1  2  3  4]

Output 2x2:
[6  8]
[3  4]
```

**Average Pooling:**
- Ambil rata-rata dari region
- Lebih smooth daripada max pooling
- Jarang dipakai sekarang

**Parameter:**
- Pool size: 2x2 atau 3x3
- Stride: biasanya sama dengan pool size

### 4. Fully Connected Layer

- Di akhir CNN sebelum output
- Flatten feature maps jadi 1D vector
- Standard neural network layers
- Output layer untuk classification

## Complete CNN Architecture

**Typical flow:**
```
Input Image
   ↓
Conv → ReLU → Pool
   ↓
Conv → ReLU → Pool
   ↓
Conv → ReLU → Pool
   ↓
Flatten
   ↓
Fully Connected → ReLU
   ↓
Output Layer (Softmax/Sigmoid)
```

**Example:**
```python
import tensorflow as tf
from tensorflow import keras

model = keras.Sequential([
    # First Conv Block
    keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),
    keras.layers.MaxPooling2D((2,2)),
    
    # Second Conv Block
    keras.layers.Conv2D(64, (3,3), activation='relu'),
    keras.layers.MaxPooling2D((2,2)),
    
    # Third Conv Block
    keras.layers.Conv2D(128, (3,3), activation='relu'),
    keras.layers.MaxPooling2D((2,2)),
    
    # Flatten and FC layers
    keras.layers.Flatten(),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(10, activation='softmax')
])
```

## Feature Maps dan Hierarchical Learning

### Low-Level Features (Early Layers)
- Edges, corners, lines
- Simple patterns
- Small receptive field

### Mid-Level Features
- Textures, patterns
- Combinations of edges
- Larger receptive field

### High-Level Features (Deep Layers)
- Object parts (mata, telinga, roda)
- Complex shapes
- Very large receptive field

**Contoh pada gambar wajah:**
- Layer 1: detect edges
- Layer 2: detect curves dan shapes
- Layer 3: detect mata, hidung, mulut
- Layer 4: detect seluruh wajah

## Padding

### Valid Padding (No Padding)
- Output lebih kecil dari input
- Pixel di edge kurang diproses
```python
keras.layers.Conv2D(32, (3,3), padding='valid')
```

### Same Padding
- Output size sama dengan input
- Tambahkan 0 di border
- Semua pixel diproses sama banyak
```python
keras.layers.Conv2D(32, (3,3), padding='same')
```

**Rumus padding untuk 'same':**
```
Padding = (Kernel - 1) / 2
```
Untuk kernel 3x3: padding = 1

## Receptive Field

**Definisi:** area pada input image yang influence satu pixel pada feature map.

**Semakin dalam layer:**
- Receptive field semakin besar
- Bisa "melihat" area lebih luas
- Capture context yang lebih luas

**Contoh:**
- Layer 1: receptive field 3x3
- Layer 2: receptive field 7x7
- Layer 3: receptive field 15x15

## Data Preprocessing untuk CNN

### 1. Normalization
```python
# Normalize pixel values ke [0, 1]
X_train = X_train / 255.0
X_test = X_test / 255.0

# Atau standardization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)
```

### 2. Resize Images
```python
from tensorflow.keras.preprocessing import image

img = image.load_img('photo.jpg', target_size=(224, 224))
img_array = image.img_to_array(img)
```

### 3. Data Augmentation
```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=20,        # Rotate up to 20 degrees
    width_shift_range=0.2,    # Shift horizontal
    height_shift_range=0.2,   # Shift vertical
    horizontal_flip=True,     # Random horizontal flip
    zoom_range=0.2,           # Random zoom
    shear_range=0.2           # Random shear
)

datagen.fit(X_train)
```

**Kenapa augmentation?**
- Increase training data variety
- Reduce overfitting
- Model jadi lebih robust
- Especially penting kalau data sedikit

## Training CNN

### 1. Loss Function

**Binary Classification:**
```python
model.compile(loss='binary_crossentropy')
```

**Multi-class Classification:**
```python
model.compile(loss='categorical_crossentropy')  # One-hot encoded
# Atau
model.compile(loss='sparse_categorical_crossentropy')  # Integer labels
```

### 2. Optimizer

**Adam (Recommended):**
```python
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
```

**SGD with momentum:**
```python
from tensorflow.keras.optimizers import SGD
optimizer = SGD(learning_rate=0.01, momentum=0.9)
model.compile(optimizer=optimizer, loss='categorical_crossentropy')
```

### 3. Callbacks

**Early Stopping:**
```python
from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

model.fit(X_train, y_train, epochs=50, callbacks=[early_stop])
```

**Model Checkpoint:**
```python
from tensorflow.keras.callbacks import ModelCheckpoint

checkpoint = ModelCheckpoint(
    'best_model.h5',
    monitor='val_accuracy',
    save_best_only=True
)
```

**Learning Rate Scheduler:**
```python
from tensorflow.keras.callbacks import ReduceLROnPlateau

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=3,
    min_lr=0.00001
)
```

## Regularization Techniques

### 1. Dropout
```python
keras.layers.Dropout(0.5)
```
- Randomly "drop" neurons during training
- Prevent overfitting
- Biasanya 0.2 - 0.5

### 2. L2 Regularization
```python
from tensorflow.keras.regularizers import l2

keras.layers.Conv2D(64, (3,3), kernel_regularizer=l2(0.001))
```

### 3. Batch Normalization
```python
keras.layers.BatchNormalization()
```
- Normalize activations
- Faster training
- Regularization effect

**Placement:**
```python
keras.layers.Conv2D(64, (3,3)),
keras.layers.BatchNormalization(),
keras.layers.Activation('relu')
```

## Evaluation

### Confusion Matrix
```python
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

cm = confusion_matrix(y_true, y_pred_classes)
sns.heatmap(cm, annot=True, fmt='d')
```

### Classification Report
```python
print(classification_report(y_true, y_pred_classes))
```
- Precision, Recall, F1-score per class
- Overall accuracy

## Common Datasets

### 1. MNIST
- 70,000 handwritten digits
- 28x28 grayscale
- 10 classes (0-9)

### 2. CIFAR-10
- 60,000 color images
- 32x32 RGB
- 10 classes (plane, car, bird, etc)

### 3. ImageNet
- 1.4 million images
- 1000 classes
- Standard benchmark

## Tips Praktis

### 1. Start Simple
- Mulai dengan architecture sederhana
- Tambahkan complexity secara bertahap
- Monitor overfitting

### 2. Experiment dengan Hyperparameters
- Learning rate
- Batch size
- Number of layers
- Number of filters

### 3. Visualize
```python
# Visualize training history
import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='validation')
plt.legend()
plt.show()
```

### 4. Use GPU
- CNN training lambat di CPU
- Google Colab free GPU
- Atau local GPU (NVIDIA)

## Key Takeaways

1. **CNN designed untuk image** dengan convolution operation
2. **Three main operations:** Convolution, Activation, Pooling
3. **Hierarchical learning:** low-level → high-level features
4. **Data augmentation penting** untuk avoid overfitting
5. **Padding dan stride** control output size
6. **Regularization** (dropout, batch norm) prevent overfitting
7. **Start simple**, experiment, dan monitor performance
