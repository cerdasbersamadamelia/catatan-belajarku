# NLP-4: Sequence-to-Sequence dengan RNN

## ğŸ¯ Apa itu RNN?

**RNN = Recurrent Neural Network**

RNN muncul untuk mengatasi masalah yang ada di Neural Network biasa:

### Masalah Neural Network Biasa:
1. **Input size harus fix (tetap)**
   - Contoh: di YOLO, gambar harus 416Ã—416
   - Kalau beda ukuran â†’ error!
   
2. **Tidak bisa handle data yang ukurannya berubah-ubah**
   - Contoh: kalimat pendek vs kalimat panjang
   - Tidak praktis untuk production

### Solusi: RNN!

RNN bisa mengolah **data sekuensial** yang:
- âœ… Ukurannya berbeda-beda (bisa 10, 20, 300 token)
- âœ… Punya urutan yang penting
- âœ… Kalau urutan dikacaukan â†’ artinya berubah

---

## ğŸ“Š Contoh Data Sekuensial

### Data Sekuensial âœ…
- ğŸ“ˆ **Harga saham** - prediksi berdasarkan 30 hari sebelumnya
- ğŸ“ **Teks/kalimat** - urutan kata penting
- ğŸ¤ **Audio** - contoh: Whisper (max 30 detik, tapi bisa bervariasi)
- ğŸŒ¡ï¸ **Suhu & cuaca** - data time series
- ğŸ§¬ **DNA** - urutan super penting!
- ğŸµ **Musik** - urutan nada

### Bukan Data Sekuensial âŒ
- ğŸ–¼ï¸ **Gambar/Image** - ukuran fix, bukan sekuensial
- ğŸ“Š **Prediksi harga rumah** - urutan tidak penting
- ğŸ”¢ **Output dari Word2Vec, TF-IDF, GloVe** - ukuran sama semua

> **Catatan:** Output embedding (Word2Vec, dll) bisa diperdebatkan. Kalau dilihat per kata memang sekuensial, tapi ukuran outputnya tetap.

---

## ğŸ§  Intuisi RNN

### Analogi: Soal SBMPTN/SNBT
```
1, 1, 2, 3, 5, 8, 13, 21, ???
```

Cara kita ngerjain:
1. Lihat 1 + 1 = 2
2. Lihat 1 + 2 = 3
3. Lihat 2 + 3 = 5
4. Dan seterusnya... **dari awal sampai akhir**

Jawabannya: **34** (karena 13 + 21 = 34)

**RNN juga kerja seperti ini** â†’ memproses dari awal sampai akhir, informasi dibawa terus!

### Contoh di NLP:
**Kalimat:** "I love spicy food"

Model harus ngerti:
- "food" â†’ objeknya
- "spicy" â†’ sifat dari food
- "love" â†’ predikatnya
- "I" â†’ subjeknya

Jadi: **"Saya suka makanan pedas"** â† model harus paham konteks keseluruhan!

---

## ğŸ”§ Cara Kerja RNN

### Komponen Penting:

1. **Time Step** = urutan ke-berapa (pertama, kedua, ketiga, dst)
2. **Input** = data yang masuk di setiap time step
3. **RNN Cell** = tempat pemrosesan
4. **Hidden State** = output dari RNN Cell (bentuknya matrix)
5. **Activation Function** = biasanya **tanh** atau **relu**

### Alur Kerja:

```
Input 1 â†’ RNN Cell â†’ Hidden State 1 â†’ dioper ke...
Input 2 + Hidden State 1 â†’ RNN Cell â†’ Hidden State 2 â†’ dioper ke...
Input 3 + Hidden State 2 â†’ RNN Cell â†’ Hidden State 3 â†’ ...
```

**Key Point:** Output dari time step sebelumnya dipakai di time step selanjutnya!

---

## ğŸ“ Formula RNN

```
h_t = f(W_hh Ã— h_(t-1) + W_xh Ã— x_t + bias)
```

Penjelasan:
- `h_t` = hidden state di time step sekarang
- `h_(t-1)` = hidden state di time step sebelumnya
- `x_t` = input di time step sekarang
- `W_hh` = weight untuk hidden state
- `W_xh` = weight untuk input
- `f` = activation function (tanh/relu)

> **Penting:** Weight (W) sama untuk semua time step!

---

## ğŸ—ï¸ Tipe-Tipe Arsitektur RNN

### 1. One-to-One
- Input: 1
- Output: 1
- Contoh: Neural Network biasa

### 2. One-to-Many
- Input: 1 (misal: gambar)
- Output: banyak (misal: caption)
- Contoh: **Image Captioning**, Text Generation

### 3. Many-to-One
- Input: banyak (misal: kalimat)
- Output: 1 (misal: sentimen)
- Contoh: **Sentiment Analysis**, Text Classification

### 4. Many-to-Many
- Input: banyak
- Output: banyak
- Contoh: **Translation**, Music Generation

---

## ğŸ”„ Encoder-Decoder Architecture

### Encoder (Many-to-One)
**Fungsi:** Memahami input dengan jelas!

Contoh input: "Nama saya adalah Agung"

Proses:
1. "Nama" â†’ diproses
2. "saya" â†’ diproses + info dari "Nama"
3. "adalah" â†’ diproses + info dari "Nama saya"
4. "Agung" â†’ diproses + info dari keseluruhan

**Output:** Hidden state terakhir yang udah ngerti semua konteks!

### Decoder (One-to-Many)
**Fungsi:** Generate output satu per satu!

Dapat hidden state dari encoder â†’ generate:
1. `<BOS>` (Beginning of Sentence) â†’ "My"
2. "My" â†’ "name"
3. "name" â†’ "is"
4. "is" â†’ "Agung"
5. "Agung" â†’ `<EOS>` (End of Sentence)

> **Makanya Chat GPT keluar satu-satu!** Bukan langsung semuanya. Ini cara kerja decoder.

---

## ğŸ“ Sequence-to-Sequence (Seq2Seq)

**Definisi:** Input berupa sequence â†’ Output juga berupa sequence

### Contoh Use Case:
- ğŸŒ **Translation** - Bahasa Indonesia â†’ Bahasa Inggris
- ğŸ’¬ **Chatbot** - Pertanyaan â†’ Jawaban
- ğŸ¤ **Speech-to-Text** - Audio â†’ Teks
- ğŸ“ **Text Summarization** - Teks panjang â†’ Ringkasan

### Arsitektur:
```
Input Sequence â†’ ENCODER â†’ Context Vector â†’ DECODER â†’ Output Sequence
```

**Context Vector** = Hidden state terakhir dari encoder yang berisi pemahaman lengkap tentang input!

---

## ğŸ”¥ Back Propagation Through Time (BPTT)

Cara RNN belajar dari kesalahan:

1. Hitung loss di setiap output
2. Kalau many-to-many â†’ loss di setiap time step dijumlahkan
3. Hitung gradient dari akhir ke awal
4. Update weight berdasarkan gradient

**Catatan:** Weight sama untuk semua time step, cuma di-update bareng!

---

## âš ï¸ Masalah RNN & Solusinya

### 1. **Vanishing Gradient** (Hidden state terlalu kecil)
Solusi:
- âœ… Ganti tanh dengan **ReLU**
- âœ… Pakai **Identity Matrix** untuk inisialisasi hidden state
- âœ… Upgrade ke **GRU** (Gated Recurrent Unit)
- âœ… Upgrade ke **LSTM** (Long Short-Term Memory)

### 2. **Exploding Gradient** (Hidden state terlalu besar)
Solusi:
- âœ… **Gradient Clipping** - batasi nilai gradient

---

## ğŸ’¡ Poin Penting

### Hidden State
- â“ **Output atau bukan?** â†’ Tergantung!
  - Bisa jadi output terakhir (many-to-one)
  - Bisa jadi operan info antar time step
  - Bentuknya: **Matrix (m Ã— n)**

### Weight di RNN
- Sama untuk semua time step
- Kalau training dari scratch â†’ diinisialisasi **random**
- Kalau pakai pre-trained â†’ ambil dari model yang sudah ada

### Fully Connected Layer
- **Optional!** Bisa dipakai, bisa tidak
- Biasanya untuk classification task

---

## ğŸ“ Contoh Praktis (Spreadsheet)

### Input: "Aku suka cokelat"

**Vocab:**
- aku, suka, tidak, cokelat, jus, `<BOS>`

**Embedding:** One-hot encoding

**Weight preference:**
- `<BOS>`: 3 (sisanya 0)
- suka: 0.1
- tidak: -0.5
- cokelat: 5 (positif)
- jus: negatif

### Proses:
**Time Step 1:** "Aku"
- Input: [1, 0, 0, 0, 0, 0]
- Hidden state sebelumnya: [0, 0, 0] (karena awal)
- Output hidden state: hasil dari tanh(W Ã— input + bias)

**Time Step 2:** "suka"
- Input: [0, 1, 0, 0, 0, 0]
- Hidden state sebelumnya: dari time step 1
- Output hidden state: hasil kalkulasi baru

**Time Step 3:** "cokelat"
- Input: [0, 0, 0, 1, 0, 0]
- Hidden state sebelumnya: dari time step 2
- Output hidden state: **Final hidden state** â†’ ini yang dipakai encoder!

---

## ğŸš€ Kesimpulan

1. **RNN** dibuat untuk data sekuensial yang ukurannya bervariasi
2. **Urutan penting** â†’ kalau ditukar, artinya berubah
3. **Hidden state** membawa informasi dari awal sampai akhir
4. **Encoder** memahami input, **Decoder** generate output
5. Model LLM (Chat GPT, dll) adalah **decoder-only** model
6. RNN bisa di-upgrade jadi **GRU** atau **LSTM** untuk performa lebih baik

---

## ğŸ“š Next Steps

- Belajar **Attention Mechanism** (materi selanjutnya)
- Skip GRU & LSTM â†’ langsung ke Attention!
- Attention adalah fondasi **Transformer** (GPT, BERT, dll)