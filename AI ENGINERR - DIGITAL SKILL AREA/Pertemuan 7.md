# Pertemuan 7: RAG (Retrieval Augmented Generation)

## ğŸ¯ Overview
Materi **serius dimulai**! Kita masuk ke framework LangChain dan bikin RAG pertama kita.

---

## ğŸ› ï¸ Tools yang Dipakai

### 1. LangChain ğŸ¦œğŸ”—
**Framework utama untuk LLM!**

**Kenapa LangChain?**
- Lebih praktis & konsisten
- Dokumentasi lengkap
- Komunitas besar

**Alternatif:**
- LlamaIndex (sintaks kurang konsisten)
- Haystack

### 2. Google Gemini ğŸ’
Model LLM yang kita pakai (gratis!)

### 3. HuggingFace ğŸ¤—
Buat **embedding** (ubah teks jadi angka)
- Gratis!
- Alternatif dari OpenAI Embedding

### 4. Vector Database ğŸ—„ï¸
Tempat nyimpen dokumen yang udah di-load
- **FAISS** (yang kita pakai - simpel!)
- PineCone
- ChromaDB
- Qdrant
- Weaviate

---

## ğŸ“š Fitur Penting LangChain

### 1ï¸âƒ£ Document Loaders ğŸ“‚

**Fungsi:** Baca dokumen yang mau dianalisis

**Format yang Didukung:**
- ğŸ“„ PDF
- ğŸ“ Word (DOCX)
- ğŸ“Š PowerPoint (PPTX)
- ğŸ“ˆ Excel & CSV
- ğŸŒ Website (pakai BeautifulSoup)
- â˜ï¸ Cloud (AWS, Azure, Google Drive)
- ğŸ“± Social Media (Twitter, Reddit)
- ğŸ’¬ Chat (Telegram, WhatsApp)

**Contoh Code:**
```python
from langchain_community.document_loaders import PyPDFLoader

# Load PDF
loader = PyPDFLoader("dokumen.pdf")
docs = loader.load()

print(docs)  # Isi dokumen
```

**Tips:**
- PDF â†’ pakai `PyPDFLoader`
- CSV â†’ pakai `UnstructuredCSVLoader` (lebih stabil)
- Website â†’ butuh install `beautifulsoup4`

### 2ï¸âƒ£ Memory ğŸ§ 

**Fungsi:** Simpan percakapan sebelumnya

**Kenapa Penting?**
Biar LLM gak kehilangan konteks!

**Contoh Tanpa Memory:**
```
User: "Bisa jelaskan jadwal bus Jakarta?"
LLM: "Jadwal bus Jakarta adalah..."

User: "Jurusannya ke mana saja?"
LLM: "Sorry, I cannot answer. Jurusan apa yang dimaksud?"
```

**Contoh Dengan Memory:**
```
User: "Bisa jelaskan jadwal bus Jakarta?"
LLM: "Jadwal bus Jakarta adalah..."

User: "Jurusannya ke mana saja?"
LLM: "Jurusannya akan pergi ke Blok M, Lebak Bulus, ..."
```

### 3ï¸âƒ£ LLM Agents ğŸ¤–

Bakal dibahas pertemuan 8!
- Eksekusi Python code
- Eksekusi SQL query
- Automasi

### 4ï¸âƒ£ Koneksi ke Tools ğŸ”Œ
- Google Gemini âœ…
- HuggingFace âœ…
- Vector Database âœ…

---

## ğŸ’¾ Vector Database Deep Dive

### Apa itu Vector Database?

**Definisi Sederhana:**
Database khusus untuk nyimpen **vektor** (angka-angka hasil embedding)

### Proses Kerja

```
Teks â†’ Embedding â†’ Vektor â†’ Disimpan di Vector DB
```

**Contoh:**
```
"Saya akan pergi" 
   â†“ embedding
[0.034, 0.045, 0.021, 0.067, ...]
   â†“
Disimpan di Vector DB
```

### Pilihan Vector Database

#### **FAISS** (Facebook AI Similarity Search) â­
- **Paling gampang** - plug & play!
- Gratis
- Lokal (gak perlu cloud)
- Cocok untuk belajar

**Setup:**
```python
pip install faiss-cpu

from langchain_community.vectorstores import FAISS
```

#### **PineCone** â˜ï¸
- Cloud-based
- Setting lebih ribet
- Butuh API key
- Cocok untuk production

#### **ChromaDB** ğŸ“¦
- Lebih canggih dari FAISS
- Ada engine SQLite bawaan
- Setup sedang

#### **Qdrant** ğŸš€
- **Paling susah!**
- Butuh Docker
- Untuk **aplikasi skala BESAR**

#### **Weaviate** ğŸŒŠ
- Setara Qdrant
- Enterprise-level
- Super kompleks

---

## ğŸ”„ Cara Kerja RAG

### Diagram Alur

```
1. Dokumen PDF/CSV/Word
        â†“
2. LangChain Loader (baca dokumen)
        â†“
3. Embedding (ubah jadi vektor)
        â†“
4. Vektor Database (simpan)
        â†“
5. User tanya â†’ Embedding juga
        â†“
6. Similarity Search (cari yang cocok)
        â†“
7. Kirim hasil + pertanyaan ke LLM
        â†“
8. LLM kasih jawaban!
```

### Detail Proses

#### **Step 1: Chunking** âœ‚ï¸
Potong dokumen jadi bagian kecil

**Contoh:**
- Dokumen 600 kata
- Chunk size = 200
- Hasil: 3 potongan (chunk 1, 2, 3)

**Code:**
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=100  # overlap biar lebih smooth
)
chunks = text_splitter.split_documents(docs)
```

#### **Step 2: Embedding** ğŸ”¢
Ubah tiap chunk jadi vektor

```python
from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
```

#### **Step 3: Simpan ke Vector DB** ğŸ’¾
```python
vectorstore = FAISS.from_documents(chunks, embeddings)
```

#### **Step 4: User tanya â†’ Similarity Search** ğŸ”
```python
query = "Bagaimana cara kerja LLM?"
# Query juga di-embedding
# Cari chunk yang paling mirip
docs_found = vectorstore.similarity_search(query, k=3)
```

#### **Step 5: Kirim ke LLM** ğŸ¤–
```python
from langchain.prompts import PromptTemplate

template = """
Based on the context: {context}
Answer the question: {question}
"""

# LLM jawab berdasarkan context + question
```

---

## ğŸ¯ Kenapa RAG Itu Bagus?

### 1. Hemat Biaya! ğŸ’°
- Input ke LLM **lebih sedikit**
- Cuma kirim chunk yang relevan
- Token lebih murah!

### 2. Jawaban Lebih Tepat! ğŸ¯
- LLM fokus ke context yang relevan
- Gak "ngalor ngidul"
- Lebih detail & jelas

### 3. Gak Perlu Retrain Model! âš¡
- Cukup update dokumen
- Model tetap sama
- Scalable!

---

## ğŸ’» Implementasi Code Lengkap

### Setup Library
```bash
pip install langchain
pip install langchain-google-genai
pip install langchain-community
pip install pypdf
pip install unstructured
pip install faiss-cpu
pip install sentence-transformers
```

### Code RAG Simpel
```python
# 1. Import semua
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA

# 2. Load dokumen
loader = PyPDFLoader("cv.pdf")
docs = loader.load()

# 3. Chunking
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=100
)
chunks = text_splitter.split_documents(docs)

# 4. Embedding
embeddings = HuggingFaceEmbeddings()

# 5. Vector Database
vectorstore = FAISS.from_documents(chunks, embeddings)

# 6. Setup LLM
llm = ChatGoogleGenerativeAI(
    model="gemini-pro",
    google_api_key="YOUR_API_KEY"
)

# 7. Query!
query = "Can you give me some insight based on my CV?"
results = vectorstore.similarity_search(query)

# 8. Generate jawaban
from langchain.prompts import PromptTemplate

template = """
Based on these documents: {docs}
Answer this question: {question}
"""

prompt = PromptTemplate(template=template, input_variables=["docs", "question"])
response = llm.invoke(prompt.format(docs=results, question=query))
print(response)
```

---

## âš ï¸ Warning & Tips

### LangChain itu Inkonsisten!
- **Dokumentasi kadang outdated**
- Code yang work hari ini, besok bisa error
- Sering update tiba-tiba
- **Jangan copas mentah-mentah!**

### Tips Debugging
1. Cek versi library: `pip list | grep langchain`
2. Kalau error â†’ cari di GitHub Issues
3. Gabung komunitas Discord/Slack
4. Tanya ChatGPT/Claude untuk fix

---

## ğŸ†š Traditional RAG vs No RAG

| Aspek | Tanpa RAG | Dengan RAG |
|-------|-----------|------------|
| **Input** | Kirim SEMUA dokumen | Cuma kirim chunk relevan |
| **Token** | Boros banget | Hemat! |
| **Jawaban** | Bisa ngalor-ngidul | Fokus & tepat |
| **Biaya** | Mahal | Murah |
| **Akurasi** | Tergantung | Lebih tinggi |

---

## ğŸš€ Use Case RAG

**Cocok untuk:**
- âœ… Chatbot FAQ perusahaan
- âœ… Analisis dokumen legal
- âœ… Research paper summarization
- âœ… Customer support automation
- âœ… Knowledge base internal

**Kurang cocok untuk:**
- âŒ Real-time data (pakai tools/API)
- âŒ Perhitungan kompleks (pakai Python Agent)
- âŒ Data yang sering berubah

---

## ğŸ“Œ Kesimpulan

âœ… LangChain = framework wajib untuk LLM  
âœ… RAG = cara bikin LLM lebih pintar & murah  
âœ… Vector DB = tempat nyimpen dokumen dalam bentuk vektor  
âœ… FAISS = pilihan terbaik untuk belajar  
âœ… Traditional RAG masih dipakai di early-stage startup  

**Next:** Advance RAG ada di pertemuan 9!

---

## â“ FAQ

**Q: BARD vs GPT?**  
A: BARD = Gemini versi lama (2023). Sekarang pakai Gemini aja.

**Q: MCP itu apa?**  
A: Model Context Protocol - untuk koneksi antar aplikasi (Claude â†’ Google Drive â†’ Database). Agak ribet, jarang dipake.

**Q: Kapan pakai Advance RAG?**  
A: Kalau kerja di BUMN/perusahaan mapan. Kalau startup kecil, traditional RAG cukup!

---

**Next:** Pertemuan 8 - LLM Agents (SQL & Python)
